---
title: ''
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
#library(gamlss)
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(rstanarm)

po_dat_long <- readRDS('data/po_dat_long.rds')
po_dat_wide <- readRDS('data/po_dat_wide.rds')

result <- 3

```

# Background

An NBA playoff series is a set of games played until one team wins four games. The maximum number of games in a series is seven since both teams could win three games apiece and the seventh game is the series-deciding game. We refer to this structure as a "best of seven series." In this presentation, we'll be modeling the impact of winning a single game on the overall probability of winning a series. We'll also examine how we can approach this model a frequentist and Bayesian perspective.

this is my result `r result`

# Data Exploration

Before building any models, let's look at the how a playoff series evolves. Interestingly, game 2 is the most correlated with winning a series than any game (besides the deciding game, of course). For a series that lasts 5 games, the team that won game 2 won the series roughly 90% of the time. For a series that lasts 6 games, the team that won game 2 won the series about 65% of the time. 

```{r}
prepender <- function(string, prefix = "Game ") paste0(prefix, string)

plot_series_correlation <- function(data, n_games_series){
  data %>%
    filter(series_length == n_games_series, win, name == 'home_team') %>%
    ggplot(aes(x = win, fill = won_series, group = won_series)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(game_num), nrow = 1, labeller = as_labeller(prepender)) +
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank()) +
    guides(fill = "none") +
    ylab('Proportion of series won') +
    scale_fill_manual(values = c('TRUE' = 'blue', 'FALSE' = 'NA'))
}


po_dat_long %>%
  plot_series_correlation(4)

po_dat_long %>%
  plot_series_correlation(5)

po_dat_long %>%
  plot_series_correlation(6)

po_dat_long %>%
  plot_series_correlation(7)

```
# Linear Probability Model - First Pass

We can model a playoff series with a linear model where we predict the outcome of a series and include indicator functions for whether a game was won or lost.

$$winSeries = \beta_0 + \Sigma_{i=1}^6 \beta_iWonGame_i + \epsilon  $$
Our first-pass model again tells us that winning game 2 has the greatest marginal effect of winning a series. The issue with this model is that a lot of series end before 6 games have been played, and any playoff series with fewer than 6 games will get thrown out of our regression.

```{r}
#basic linear probability

po_dat_wide %>%
  lm(data= ., won_series ~ win_game_1 + win_game_2 + win_game_3 + win_game_4 + win_game_5 + win_game_6) %>%
  broom::tidy() 
```

For the next pass of the model, we restrict the sample to 4, 5, 6 and 7 game series and estimate separate regressions for each of those series lengths. We run into estimation problems galore with this approach. For a 4 game series, winning any single game means the series is already decided. And for a 5 game series, once we know the first 4 games and know that the series is 5 games long, we know perfectly well which team will win the series.

```{r}
po_dat_wide %>%
  filter(series_length == 4) %>%
  lm(data= ., won_series ~ win_game_1 + win_game_2 + win_game_3 + p_win_game_1) %>%
  broom::tidy() 


po_dat_wide %>%
  filter(series_length == 5) %>%
  lm(data= ., won_series ~  win_game_1 + win_game_2 + win_game_3  + p_win_game_1) %>%
  broom::tidy() 
  
po_dat_wide %>%
  filter(series_length == 6) %>%
  lm(data= ., won_series ~ win_game_1 + win_game_2 + win_game_3 + win_game_4 + p_win_game_1) %>%
  broom::tidy() 


po_dat_wide %>%
  filter(series_length == 7) %>%
  lm(data= ., won_series ~ win_game_1 + win_game_2 + win_game_3 + win_game_4 + win_game_5  + p_win_game_1) %>%
  broom::tidy() 
```

There is probably a more clever regression framework that would help explain us examine playoff series, but a Bayesian analysis sidesteps many of our previous issues.


# Reframing the Problem

The negative binomial distribution is based on an experiment satisfying the following conditions:

1. The experiment consists of a sequence of independent trials.
2. Each trial can result in either a success or a failure.
3. The probability of success $p$ is constant from trial to trial
4. The experiment continues until a total of $r$ successes have been observed

$$nb(k; r, p) \equiv \Pr(X = k) = \binom{k+r-1}{r-1} (1-p)^kp^r$$

Consider nb(x > 3; r = 7, p = .55) = P(X > 3), the probability that at least 3 losses occur before the 7th game and the probability of winning each game is 55%.

`1 - pnbinom(3, 7, 1 - .55, lower.tail = FALSE) = .6082 `

We should think about the outcomes of an NBA series as a negative binomial distribution. Given $r$ remaining games in the series and probability of winning a single game $p$, what is the likelihood that you win $k$ games? 

# The binomial model for p

We can model $p$ using a generalized linear model (maybe this is overkill and we just talk about the logistic regression)

The GLM consists of three elements:
1. An exponential family of probability distributions.
2. A linear predictor $\eta = X \beta$
3. A link function $g$ such that $E(Y \mid X) = p = g^{-1}(\eta)$

The logistic regression we choose after playing $n$ games is 

$$p=\frac{\exp(\beta_0 + \sum_{i=1}^n \beta_iWonGame_i)}{1 + \exp(\beta_0 + \sum_{i=1}^n \beta_iWonGame_i)}$$
* probably skip the beta-distribution stuff since it's only a 30-minute presentation



```{r}
n_games <- 6
games_played <- 2

build_binomial_model <- function(n_games, games_played, model_type = 'normal'){
  game_series <- po_dat_wide %>%
  filter(series_length == n_games) %>% select_if(~sum(!is.na(.)) > 0)

  win_col_names <- grep("^win_game_", names(game_series), value = TRUE)[1:games_played]
  
  form <- reformulate(c( win_col_names),  response = "won_series")

  model <- NULL
  
  if(model_type == 'normal')
    model <- glm(form , data = game_series, family = binomial())
  
  if(model_type == 'beta')
    model <- gamlss::gamlss(form, data = game_series, link = BB())
  
  if(model_type == 'bayes')
    model <- stan_glm(form, data = game_series, family = binomial(link = 'logit'))
  
  model
  
}

beta_normal_model <- build_binomial_model(n_games, games_played, 'normal')
beta_binomial_model <- build_binomial_model(n_games, games_played, 'beta')

predict_data <- expand.grid(replicate(games_played, c(TRUE, FALSE), simplify = FALSE)) %>%
  magrittr::set_colnames(win_col_names) %>%
  unite(col = id, everything(), remove = FALSE) %>%
  mutate(id = reduce2(c('TRUE', 'FALSE', '_'), c('W', 'L', ''),  .init = id, str_replace_all)
         , n_wins = str_count(id, pattern = "W")
         , n_losses = games_played - n_wins
         , p_win_game_1 = .5)

fitted_distributions <- predict_data %>%
  mutate(fitted_beta = predict(beta_binomial_model, newdata = predict_data %>% select(-id, -n_wins, -n_losses))
         , fitted_normal = predict(normal_binomial_model, predict_data %>% select(-id), type = 'response')) %>%
  mutate(sigma_beta = exp(beta_binomial_model$sigma.coefficients)
         , sigma_normal = exp(sigma(normal_binomial_model))
         , alpha = fitted_beta/sigma_beta
         , beta = (1-fitted_beta)/sigma_beta
         , posterior_probability = 1- pnbinom(q = 3 - n_losses, size = 7 - games_played, prob = 1 - fitted_beta, lower.tail = FALSE) ) 

#how prior distribution of win probability changes 
fitted_distributions %>%
  crossing(x = seq(-5, 5, length.out = 100)) %>%
  mutate(prior_density_beta = dbeta(x, alpha, beta)
         , prior_density_normal = dnorm(x, fitted_normal, sd = sigma_normal)) %>%
  ggplot(aes(x = x, y = prior_density_normal, colour = id)) +
  geom_line()

```

# Bayesian Regression

* same setup as logistic regression
* how to integrate ELO p_win_game_1?
* write-up on advantages of bayes methodology

```{r}

beta_normal_model_bayes <- build_binomial_model(n_games, games_played, 'bayes')

bayes_coefs %>% View

summary(bayes_logistic_model)
prior_summary(bayes_logistic_model)
broom.mixed::tidy(bayes_logistic_model)
posterior_interval(bayes_logistic_model)

bayes_coefs <- map_df(1:5 ,function(games_played){
  model <- build_binomial_model(6, games_played, 'bayes')
  broom.mixed::tidy(model)
}, .id = 'games_played')

#posterior predict generates a bunch of simulations, so the prediction is the proportion of simulations that are success
bayes_proportion <- posterior_predict(bayes_logistic_model) %>% 
  data.frame() %>%
  pivot_longer(everything()) %>%
  group_by(name) %>%
  summarise(prop = sum(value)/n()) %>%
  mutate(n = readr::parse_number(name)) %>%
  arrange(n) %>%
  pull(prop)

beta_normal_model_bayes$data %>%
  ungroup() %>%
  mutate(bayes_prop = bayes_prop)

```
